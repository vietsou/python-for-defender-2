{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec957cc0-284a-40e5-a666-80fc169787e7",
   "metadata": {},
   "source": [
    "# 4-3: Scraping\n",
    "\n",
    "APIs are great, but they can't do everything. Sometimes we need to use other methods to analyze content. Specifically, I want to discuss how we analyze web content either for disposition or to extract useful information from it. Put another way, how can we make Python read and understand websites programmatically?\n",
    "\n",
    "The answer is BeautifulSoup.\n",
    "\n",
    "## Funny Name, Invaluable Library\n",
    "\n",
    "This may be one of the most important Python modules I've ever used. [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is a parser for HTML/XML files. With it, we can do some pretty incredible things. Let's start by learning the basic shape of BS4.\n",
    "\n",
    "We begin by importing the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f845082b-f8b5-4f1c-8352-88eefca01786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and setup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3067fa0a-ce0c-4062-9ba7-56a811694398",
   "metadata": {},
   "source": [
    "Now we need to give it something to parse. Once again, My simple blog serves well for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c0af153-5879-4709-9855-7e955f43a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "document: str = requests.get(\"https://taggart-tech.com\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82cc01f9-10ac-4410-88c2-f53f8d50bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e0ffe0-46b4-4c89-b2db-6d19c21c5d3d",
   "metadata": {},
   "source": [
    "`soup` is a complex object, but it helps us parse the HTML in valuable ways. Let's grab the page title as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53b9d42b-7f2a-48d4-9cf9-2a4f468aebc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Taggart Tech</title>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the title\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35af7f1-17cc-49e8-bf24-87cfde328854",
   "metadata": {},
   "source": [
    "At first glance, that might seem like a string of HTML. But look carefully: no quotes. No, this is something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c26184e0-8b1c-4431-bcc8-f4e19171d1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are you, soup.title?\n",
    "type(soup.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a422ff4f-2324-4070-8eb6-edd66402374b",
   "metadata": {},
   "source": [
    "As a `Tag` object, there's more we can do with the title. Including extract its text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2863689f-3754-49d4-b92d-bcebe9ed13f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Taggart Tech'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16860950-388b-4c92-9e89-f4e86dc116a8",
   "metadata": {},
   "source": [
    "Now _that's_ a string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8444751-71d3-435f-ab34-1dca2da31d16",
   "metadata": {},
   "source": [
    "## Seeking through HTML\n",
    "\n",
    "The power of BeautifulSoup comes from the ability to slice through a document for exactly what we want. For example, say we watned to pull all the links (`<a>`) tags out of the page. We could use the `.find_all()` method to do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75546107-2281-44e7-a843-fc840e070403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"\" href=\"https://taggart-tech.com\" itemprop=\"url\">\n",
       "<span itemprop=\"name\">Home\n",
       "                                </span></a>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab links\n",
    "links = soup.find_all(\"a\")\n",
    "# Show the first one\n",
    "links[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349f0cf7-a541-4809-8fb2-bba592db0fc2",
   "metadata": {},
   "source": [
    "Again, since the results are `Tag` objects, we can destructure this a little bit more to access just the `href` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cc9d5a5-a4a2-443f-9750-99b995c0004c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://github.com/mttaggart',\n",
       " 'https://joinmastodon.org',\n",
       " 'https://taggart-tech.com',\n",
       " 'https://taggart-tech.com/about',\n",
       " 'https://taggart-tech.com/categories',\n",
       " 'https://taggart-tech.com/chrome-extensions/',\n",
       " 'https://taggart-tech.com/cybersecurity-degrees/',\n",
       " 'https://taggart-tech.com/fear-teacher/',\n",
       " 'https://taggart-tech.com/page/2/',\n",
       " 'https://taggart-tech.com/quasar-electron/',\n",
       " 'https://taggart-tech.com/tags',\n",
       " 'https://taggart-tech.com/the-federated-future/',\n",
       " 'https://twitch.tv/mttaggart',\n",
       " 'https://twitter.com/mttaggart',\n",
       " 'https://www.youtube.com/watch?v=dx6WrKpj8HE',\n",
       " 'https://youtu.be/gUWfDyx9f0s?t=9361'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get just the hrefs \n",
    "# And use (set) to dedup\n",
    "hrefs = set([ l[\"href\"] for l in links ])\n",
    "hrefs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2db24c-2c2d-40a6-bb50-21acfefbe19d",
   "metadata": {},
   "source": [
    "The syntax of all the ways to work with BeautifulSoup objects can be a bit confusing. I almost always have the [documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) up while I'm working with it. Still, the capabilities are quite impressive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ce329-9363-408e-9e09-4a3120ab7d54",
   "metadata": {},
   "source": [
    "## An RSS Reader?! Yes.\n",
    "\n",
    "We can heal the wounds. We can create an RSS Feed processor in Python. \n",
    "\n",
    "Did you know CISA's [US-CERT](https://www.cisa.gov/uscert/ncas) has an RSS Feed? It's true! It's a solid way to get basic information about emerging threats.\n",
    "\n",
    "So let's build handler and parser to:\n",
    "\n",
    "1. Process the Feed\n",
    "2. Produce Headlines\n",
    "3. Get data from linked articles.\n",
    "\n",
    "To start, let's set up the URL of the RSS feed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b002063-53b2-48cf-b12d-c58125efc9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set feed URL\n",
    "feed_url: str = \"https://www.cisa.gov/uscert/ncas/current-activity.xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833f6437-c39d-4d16-ae5a-24bc4405b982",
   "metadata": {},
   "source": [
    "Then we'll download the data with `requests` and use BeautifulSoup to parse the XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "437a655f-9573-449d-991a-96735881f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the XML\n",
    "us_cert_xml = requests.get(feed_url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14285cde-331f-4065-a67f-efeec705b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soupify the XML\n",
    "soup = BeautifulSoup(us_cert_xml, \"xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f0cd6-e2ba-4b3d-b2a7-a4867364a94d",
   "metadata": {},
   "source": [
    "Now that we have the parsed XML, we need to understand the structure. Every article is inside of an `<item>` tag. So to get all the items, we will use the `.find_all()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "847099a4-af3a-4d0f-8c25-24162426022f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<item>\n",
       "<title>CISA Updates Advisory on Threat Actors Exploiting Multiple CVEs Against Zimbra Collaboration Suite</title>\n",
       "<link>https://us-cert.cisa.gov/ncas/current-activity/2022/10/19/cisa-updates-advisory-threat-actors-exploiting-multiple-cves</link>\n",
       "<description>Original release date: October 19, 2022&lt;br/&gt;&lt;p&gt;CISA and the Multi-State Information Sharing &amp;amp; Analysis Center (MS-ISAC) have updated joint Cybersecurity Advisory &lt;a href=\"https://www.cisa.gov/uscert/ncas/alerts/aa22-228a\"&gt;AA22-228A: Threat Actors Exploiting Multiple CVEs Against Zimbra Collaboration Suite&lt;/a&gt;, originally released August 16, 2022. The advisory has been updated to reference the addition of a new Malware Analysis Report, &lt;a href=\"https://www.cisa.gov/uscert/ncas/analysis-reports/ar22-292a\"&gt;MAR-10398871.r1.v2&lt;/a&gt;.&lt;/p&gt;\n",
       "\n",
       "&lt;p&gt;CISA encourages organizations to review the latest update to AA22-228A and apply the recommended mitigations.&lt;/p&gt;\n",
       "\n",
       "            &lt;div class=\"field field--name-body field--type-text-with-summary field--label-hidden field--item\"&gt;&lt;p class=\"privacy-and-terms\"&gt;This product is provided subject to this &lt;a href=\"https://us-cert.cisa.gov/privacy/notification\"&gt;Notification&lt;/a&gt; and this &lt;a href=\"https://www.dhs.gov/privacy-policy\"&gt;Privacy &amp;amp; Use&lt;/a&gt; policy.&lt;/p&gt;\n",
       "&lt;/div&gt;\n",
       "      </description>\n",
       "<pubDate>Wed, 19 Oct 2022 14:58:00 +0000</pubDate>\n",
       "<dc:creator>CISA</dc:creator>\n",
       "<guid isPermaLink=\"false\">18080 at https://us-cert.cisa.gov</guid>\n",
       "</item>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all <item>s\n",
    "items = soup.find_all(\"item\")\n",
    "\n",
    "# Review one for structure\n",
    "items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca21ac0-3c19-41d0-b439-6e3ceebe31e7",
   "metadata": {},
   "source": [
    "So each `item` has a `title`, a `link`, and a `description`. That's enough to get us going. But how should we handle it?\n",
    "\n",
    "One option might be the `HTML` Widget. We can convert each item into some crafted HTML.\n",
    "\n",
    "Did you notice the weird characters in `description`? The HTML tags have been escaped to make them XML safe. To reverse that, we'll need the `html.unescape()` method. A simple import, but worth mentioning. \n",
    "\n",
    "Okay, let's write a function to HTMLify the items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9f3eef33-9c47-4c32-8f7d-187c3ff74965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from html import unescape\n",
    "\n",
    "def html_item(item) -> str:\n",
    "    \"\"\"\n",
    "    Converts an XML item into HTML\n",
    "    \"\"\"\n",
    "    item_html = f\"<a href=\\\"{item.link.text}\\\"><h2>{item.title.text}</h2></a>\"\n",
    "    item_html += f\"<p>{unescape(item.description.text)}</p>\"\n",
    "    return item_html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02392db-35ee-49d3-833e-31abc450117c",
   "metadata": {},
   "source": [
    "And now, to display the results. I'm adding a little `<style>` to the proceedings to make the links pop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9275f4ef-3d2e-4f8b-97e0-6b2eccb8a4d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea9a618a146439c963e5c16925ebc8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n<style>\\n    a {\\n        color: #9580ff;\\n        font-weight: boldest;\\n    }\\n</style>\\n<a hr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "style_html = \"\"\"\n",
    "<style>\n",
    "    a {\n",
    "        color: #9580ff;\n",
    "        font-weight: boldest;\n",
    "    }\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "# We'll just do the first 3 items \n",
    "html_items = [ html_item(i) for i in items[:3] ]\n",
    "html_str = \"\".join(html_items)\n",
    "\n",
    "html_out = widgets.HTML(value=style_html + html_str)\n",
    "display(html_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381a2058-0ddc-428c-a4e7-c95e1f84dcd1",
   "metadata": {},
   "source": [
    "Now this is one way to parse and use this data, but I'm sure you can come up with others—and other feeds to connect to. Get creative!\n",
    "\n",
    "In our final lesson in this section, we'll build a webpage analyzer to assess for potential malware!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
